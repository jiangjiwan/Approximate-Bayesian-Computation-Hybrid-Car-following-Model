{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "174c70c8-df96-43a8-94d7-38bb449e2c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hybird Model\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "from traj_data import *\n",
    "from models import *\n",
    "from approximate_bayesian_new import *\n",
    "#import seaborn as sns\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import timeit\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd073d18-5d0d-493f-85ec-cc2120a6aa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_set = 'CIVIC'\n",
    "#num_samples = 1000001\n",
    "num_samples = 1000001\n",
    "max_para_dim = 10\n",
    "max_model_id = 20\n",
    "max_score = 9999\n",
    "ts = 0.1\n",
    "result = np.empty((0,max_para_dim+5))\n",
    "#for model_name in [,'IDM', 'FVM', 'GFM', 'HL', 'LLCS', 'OVM','MPC','FVM_CS','Newell','IDM_CS''FVM_SIGMOID']:\n",
    "#for model_name in ['OVM']:  #,   'MPC' \n",
    "#for model_name in ['IDM']:\n",
    "#for model_name in ['LL','IDM', 'FVM', 'GFM', 'HL', 'LLCS', 'OVM','MPC']: \n",
    "#for model_name in ['IDM', 'FVM', 'GFM',  'OVM']: #, 'HL'\n",
    "for model_name in ['IDM', 'FVM', 'GFM',  'OVM', 'HL']: #, 'HL'\n",
    "    #result_path = '../cross_ngsim/Cross_result_%d_%s_%s_1.csv' % (num_samples, model_name, Data_set)\n",
    "    result_path = '../New_15_result_%d_%s_%s_new3.csv' % (num_samples, model_name, Data_set)\n",
    "    temp_result =pd.read_csv(open(result_path, \"rb\"), delimiter=\",\", skiprows=1, nrows=900000)\n",
    "    #temp_result = np.loadtxt(open(result_path, \"rb\"), delimiter=\",\", skiprows=1)\n",
    "    #result_1w = temp_result[0:9999, :]\n",
    "    # print(temp_result)\n",
    "    result = np.append(result, temp_result, axis = 0)\n",
    "    #result = np.append(result_1w, result_1w, axis = 0)\n",
    "#np.savetxt('2vs2_result_%s.csv' % Data_set, result, delimiter = \",\", fmt='%.4f')\n",
    "#np.savetxt('merged_result2_%s.csv' % Data_set, result, delimiter = \",\", fmt='%.4f')\n",
    "np.savetxt('Single_result_%s_%s.csv' % (Data_set,  model_name), result, delimiter = \",\", fmt='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "756707d3-3795-4b4d-a174-66a37844d119",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class particle_evaluation:\n",
    "    def __init__(self, result_file, training_file, testing_file):\n",
    "        self.raw_result = np.loadtxt(open(result_file, \"rb\"), delimiter=\",\", skiprows=0)\n",
    "        self.training_file = training_file\n",
    "        self.testing_file = testing_file\n",
    "        self.load_testing_data()\n",
    "    \n",
    "    def load_testing_data(self):\n",
    "        training_vecs = read_traj(self.training_file, ts)\n",
    "        testing_vecs = read_traj(self.testing_file, ts)\n",
    "        self.training_vec_leader = training_vecs[0]\n",
    "        self.training_vec_follower = training_vecs[1]\n",
    "        self.testing_vec_leader = testing_vecs[0]\n",
    "        self.testing_vec_follower = testing_vecs[1]\n",
    "        \n",
    "        self.training_samples = self.training_vec_leader.num_veh\n",
    "        self.testing_samples = self.testing_vec_leader.num_veh\n",
    "        self.simu_len =  self.training_vec_leader.sample_points\n",
    "        \n",
    "    def processing_particles_result(self, weight = [0.5, 0.3, 0.2]):\n",
    "\n",
    "        self.error_weight = weight\n",
    "        score = np.zeros([self.raw_result.shape[0], 1])\n",
    "        score[:,0] = self.raw_result[:,0] * weight[0]  + self.raw_result[:,1]* weight[1] + self.raw_result[:,2] * weight[2]\n",
    "        result_with_score = np.concatenate((score, self.raw_result), axis = 1)\n",
    "        self.result_with_score = result_with_score[result_with_score[:,0].argsort()]\n",
    "        #np.savetxt('hybrid_result_with_score_%s.csv' % (Data_set), self.result_with_score, delimiter = \",\", fmt='%.4f')\n",
    "        np.savetxt('hybrid_result_with_score_cross_new_%s.csv' % (Data_set), self.result_with_score, delimiter = \",\", fmt='%.4f')\n",
    "        \n",
    "        \n",
    "        self.result_with_score_by_traj = {}\n",
    "        for i in range(self.training_samples):\n",
    "            self.result_with_score_by_traj[i] = np.empty((0,self.result_with_score.shape[1]))\n",
    "        for i in range(self.result_with_score.shape[0]):\n",
    "            #if self.result_with_score[i, 4] != math.inf:\n",
    "            if self.result_with_score[i, 4] != math.inf:\n",
    "                traj_id = int(self.result_with_score[i, 4])\n",
    "            if self.result_with_score[i, 0] < max_score:\n",
    "                self.result_with_score_by_traj[traj_id] = np.append(self.result_with_score_by_traj[traj_id], [self.result_with_score[i, :]], axis = 0)\n",
    "        #for i in range(self.training_samples):\n",
    "            #np.savetxt('hybrid_result_by_traj_%d_%s.csv' % (i, Data_set), self.result_with_score_by_traj[i], delimiter = \",\", fmt='%.4f')\n",
    "\n",
    "    def cutting_particles(self, epsilon_cut = 5):\n",
    "        filtered_by_traj = {}\n",
    "        model_selected_profile = np.zeros(max_model_id)\n",
    "        self.particles_group_by_traj = np.empty((0,self.result_with_score.shape[1]))\n",
    "        for i in range(self.training_samples):\n",
    "            threshold = self.result_with_score_by_traj[i][epsilon_cut,0]\n",
    "            filtered_by_traj[i] = self.result_with_score_by_traj[i][self.result_with_score_by_traj[i][:,0]<threshold]\n",
    "            self.particles_group_by_traj = np.append(self.particles_group_by_traj, filtered_by_traj[i], axis = 0)\n",
    "        self.n_particles = self.particles_group_by_traj.shape[0]\n",
    "        self.particles_group_by_traj = self.particles_group_by_traj[self.particles_group_by_traj[:,0].argsort()]\n",
    "        for i in range(self.n_particles):           \n",
    "            model_selected_profile[int(self.particles_group_by_traj[i,5])] += 1\n",
    "        self.particles = self.particles_group_by_traj\n",
    "        np.savetxt(\"filtered_particles_15new_%s_%s.csv\" % (Data_set, model_name), self.particles_group_by_traj, delimiter = ',', fmt = '%.4f')\n",
    "        #np.savetxt(\"model_selection_profile2_%s.csv\" % Data_set, model_selected_profile, delimiter = ',', fmt = '%d')\n",
    "        #np.savetxt(\"model_selection2V2_profile_%s.csv\" % Data_set, model_selected_profile, delimiter = ',', fmt = '%d')\n",
    "        \n",
    "    def cal_particle_score(self, leader, follower, particle):\n",
    "        my_model = MODEL_LIST[int(particle[5])](ts)\n",
    "        #print(my_model.strPara)\n",
    "        simu_follower = my_model.single_simulation(leader, follower, particle[6:])\n",
    "        \n",
    "        #print(particle[6:])\n",
    "        \n",
    "        #TrajPair(leader,follower).show('a')\n",
    "        #print(simu_follower.position)\n",
    "        #print(simu_follower.speed)\n",
    "        #print(follower.acceleration)\n",
    "        \n",
    "    \n",
    "        error = simu_follower.distance(follower)\n",
    "        \n",
    "        return (self.error_weight[0]*error[0]+ self.error_weight[1]*error[1]+ self.error_weight[2]*error[2], simu_follower)\n",
    "    \n",
    "    def select_mean_particle(self):\n",
    "        \n",
    "        using_parallel = False  \n",
    "        \n",
    "        if using_parallel:\n",
    "            \n",
    "            num_cores = multiprocessing.cpu_count()\n",
    "            batchsize = int(np.floor(self.training_samples / num_cores))\n",
    "            n_batch = int(np.floor(self.training_samples / batchsize))\n",
    "            \n",
    "            print('CPU cores using: %d' % num_cores)\n",
    "            print('%d Trajectories truncated' % (self.testing_samples-n_batch * batchsize))\n",
    "            \n",
    "            def process(n):\n",
    "                grouped_score = np.zeros([self.n_particles,batchsize])\n",
    "                for bn in range(batchsize):\n",
    "                    loc = n*batchsize + bn                  \n",
    "                    for i in range(self.n_particles):\n",
    "                        grouped_score[i,bn],_ = self.cal_particle_score(self.training_vec_leader.veh[loc], self.training_vec_follower.veh[loc], self.particles[i,:])\n",
    "                return grouped_score\n",
    "            parallel_result_score = Parallel(n_jobs=num_cores)(delayed(process)(n) for n in range(n_batch))\n",
    "            \n",
    "            final_score = np.zeros([self.n_particles,n_batch*batchsize])\n",
    "            for n in range(n_batch):\n",
    "                for bn in range(batchsize):\n",
    "                    loc = n*batchsize + bn\n",
    "                    for i in range(self.n_particles): \n",
    "                        final_score[i, loc] = parallel_result_score[n][i,bn]\n",
    "            \n",
    "            best_particle = -1\n",
    "            best_score = np.inf\n",
    "            for i in range(self.n_particles): \n",
    "                particle_score = 0\n",
    "                for j in range(n_batch*batchsize):\n",
    "                    particle_score+=final_score[i,j]\n",
    "                particle_score = particle_score / (n_batch*batchsize)\n",
    "                \n",
    "                if particle_score < best_score:\n",
    "                    best_score = particle_score \n",
    "                    best_particle = i\n",
    "\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            best_particle = -1\n",
    "            best_score = np.inf\n",
    "            for i in range(self.n_particles):\n",
    "                particle = self.particles[i,:]\n",
    "                particle_score = 0\n",
    "                for j in range(self.training_samples):\n",
    "                    temp,_ = self.cal_particle_score(self.training_vec_leader.veh[j], self.training_vec_follower.veh[j], particle)\n",
    "                    particle_score += temp\n",
    "                particle_score = particle_score / self.training_samples\n",
    "\n",
    "                if particle_score < best_score:\n",
    "                    best_score = particle_score \n",
    "                    best_particle = i\n",
    "                    \n",
    "        self.mean_particle = best_particle\n",
    "        self.mean_particle_test_score = self.particle_score_array[best_particle,:].mean()\n",
    "        print('best particle: %d' % best_particle)\n",
    "        print('best particle score: %f' % best_score)\n",
    "        return best_particle, best_score\n",
    "    \n",
    "    def particle_testing(self):\n",
    "        self.particle_score_array = np.zeros([self.n_particles, self.testing_samples])\n",
    "        self.sim_traj = {}\n",
    "        num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "        batchsize = int(np.floor(self.testing_samples / num_cores))\n",
    "        n_batch = int(np.floor(self.testing_samples / batchsize))\n",
    "        \n",
    "        using_parallel = False    ####switch\n",
    "        \n",
    "        if using_parallel:\n",
    "            print('CPU cores using: %d' % num_cores)\n",
    "            print('%d Trajectories truncated' % (self.testing_samples-n_batch * batchsize))\n",
    "            def process(n):\n",
    "                grouped_score = np.zeros([self.n_particles,batchsize])\n",
    "                grouped_sim_traj = {}\n",
    "                for bn in range(batchsize):\n",
    "                    loc = n*batchsize + bn                  \n",
    "                    for i in range(self.n_particles):\n",
    "                        grouped_score[i,bn],grouped_sim_traj[i,bn] = self.cal_particle_score(self.testing_vec_leader.veh[loc], self.testing_vec_follower.veh[loc], self.particles[i,:])\n",
    "                return grouped_score,grouped_sim_traj\n",
    "    \n",
    "            parallel_result_zipped = Parallel(n_jobs=num_cores)(delayed(process)(n) for n in range(n_batch))\n",
    "            parallel_result_score, parallel_result_sim_traj = zip(*parallel_result_zipped)\n",
    "            \n",
    "            for n in range(n_batch):\n",
    "                for bn in range(batchsize):\n",
    "                    loc = n*batchsize + bn\n",
    "                    for i in range(self.n_particles): \n",
    "                        self.particle_score_array[i, loc] = parallel_result_score[n][i,bn]\n",
    "                        self.sim_traj[i, loc] = parallel_result_sim_traj[n][i,bn]\n",
    "        #self.testing_vec_leader\n",
    "        else:\n",
    "            for i in range(self.n_particles):\n",
    "                particle = self.particles[i,:]            \n",
    "                for j in range(self.testing_samples):\n",
    "                    self.particle_score_array[i, j], self.sim_traj[i, j] = self.cal_particle_score(self.testing_vec_leader.veh[j], self.testing_vec_follower.veh[j], particle)\n",
    "                    \n",
    "                    ####Debug\n",
    "                    #if i == 0 and j==0:\n",
    "                        #TrajPair(self.testing_vec_leader.veh[j],self.testing_vec_follower.veh[j]).show('s')\n",
    "                        #print(particle)\n",
    "                        #print(self.particle_score_array[i, j])\n",
    "                    \n",
    "        #np.savetxt('Score_array_%s_%s.csv' % (Data_set, model_name), self.particle_score_array, delimiter = ',', fmt ='%.4f')\n",
    "                \n",
    "    def cal_wasserstein_dist(self):\n",
    "        C = self.particle_score_array\n",
    "        #print(C)\n",
    "        #np.savetxt('Score_array.csv', C, delimiter = ',', fmt ='%.4f')\n",
    "        X = cp.Variable(C.shape)\n",
    "        E1 = np.ones(C.shape[1])\n",
    "        E0 = np.ones(C.shape[0])\n",
    "        prob = cp.Problem(cp.Minimize(cp.trace(C @ X.T)), \n",
    "                         [X @ E1 == 1/C.shape[0],\n",
    "                         X.T @ E0 == 1/C.shape[1],\n",
    "                         X >= 0,\n",
    "                         X <= 1])\n",
    "        prob.solve()\n",
    "        #prob.solve(solver=cp.SCS, verbose = True)\n",
    "        #prob.solve(solver = cp.SCIPY, verbose = True)\n",
    "        self.X = X.value\n",
    "        self.wasserstein_dist = prob.value \n",
    "        #np.savetxt(\"Probability array_IDM.csv\",X.value, delimiter = ',', fmt ='%.4f')\n",
    "\n",
    "#### Stochastic Metrics        \n",
    "    def cal_minimum_dist(self):\n",
    "        C = self.particle_score_array\n",
    "        X = cp.Variable(C.shape)\n",
    "        E1 = np.ones(C.shape[1])\n",
    "        E0 = np.ones(C.shape[0])\n",
    "        beta = 0\n",
    "        prob = cp.Problem(cp.Minimize(cp.trace(C @ X.T)), \n",
    "                         [#X @ E1 == 1/C.shape[0],    ####shape 0: row\n",
    "                         X.T @ E0 == 1/C.shape[1],\n",
    "                         X >= 0,\n",
    "                         X <= 1])\n",
    "        prob.solve()\n",
    "        self.X = X.value\n",
    "        self.minimum_dist = prob.value \n",
    "       \n",
    "\n",
    "    def cal_five_percent_dist(self):\n",
    "        C = self.particle_score_array\n",
    "        X = cp.Variable(C.shape)\n",
    "        E1 = np.ones(C.shape[1])\n",
    "        E0 = np.ones(C.shape[0])\n",
    "        beta = 0.15\n",
    "        prob = cp.Problem(cp.Minimize(cp.trace(C @ X.T)), \n",
    "                         [X @ E1 >= beta*(1/C.shape[0]),\n",
    "                         X.T @ E0 == (1/C.shape[1]),\n",
    "                         X >= 0,\n",
    "                         X <= 1])\n",
    "        prob.solve()\n",
    "        self.X = X.value\n",
    "        self.five_percent_dist = prob.value \n",
    "        \n",
    "           \n",
    "        \n",
    "#### New metric        \n",
    "    #def expectation(self):\n",
    "\n",
    "        #C = self.particle_score_array\n",
    "        #C_bar = C.mean(axis = 1)\n",
    "        #E1 = np.ones(C_bar.shape[0])\n",
    "        #Solution = np.zeros(len(C_bar))\n",
    "        #Number = np.zeros(len(C_bar))\n",
    "        \n",
    "        #Y = cp.Variable(C_bar.shape, integer = True)\n",
    "        #for N in range (1, len(C_bar)):\n",
    "        ##N = 2\n",
    "            #prob = cp.Problem(cp.Minimize(cp. sum((C_bar @ Y)/N)),\n",
    "                               # [Y @ E1 == N,\n",
    "                                 #Y >= 0,\n",
    "                                # Y <= 1])\n",
    "            #prob.solve()\n",
    "            #self.Y = Y.value\n",
    "            #self.expectation = prob.value\n",
    "            #Solution[N - 1] =prob.value\n",
    "            ##print(Y.value)\n",
    "            ##solu = np.concatenate(Solution, Number)\n",
    "            \n",
    "        ##np.savetxt('Exp.csv', Solution, delimiter = ',', fmt ='%.4f')\n",
    "        ##return (min(Solution))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_particle_quantile(self, veh, n):\n",
    "        #  return the index of the nth minimum score for a particular vehicle      \n",
    "        score = self.particle_score_array[:, veh]\n",
    "        val = np.partition(score,n)[n]\n",
    "        return np.where(score == val)[0][0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_veh_quantile(self, particle, n):\n",
    "        #  return the index of the nth minimum score for a particular vehicle      \n",
    "        score = self.particle_score_array[particle, :]\n",
    "        val = np.partition(score,n)[n]\n",
    "        return np.where(score == val)[0][0]\n",
    "    \n",
    "    # def show_best_particle(self):\n",
    "    #     for j in range(self.testing_samples):\n",
    "    #         plt.plot(self.sim_traj[self.best_particle, j].position - self.testing_vec_follower.veh[j].position, 'b.', markersize=.03)\n",
    "    #     plt.show()\n",
    "    \n",
    "    def error_plot(self, particle, vehicle, d_type, marker, markersize):\n",
    "        if d_type == 'p':\n",
    "            plt.plot(self.sim_traj[particle, vehicle].position - self.testing_vec_follower.veh[vehicle].position, marker, markersize=markersize)\n",
    "            return(self.sim_traj[particle, vehicle].position - self.testing_vec_follower.veh[vehicle].position)\n",
    "        if d_type == 's':\n",
    "            plt.plot(self.sim_traj[particle, vehicle].speed - self.testing_vec_follower.veh[vehicle].speed, marker, markersize=markersize)\n",
    "            return(self.sim_traj[particle, vehicle].speed - self.testing_vec_follower.veh[vehicle].speed)\n",
    "        if d_type == 'a':\n",
    "            plt.plot(self.sim_traj[particle, vehicle].acceleration - self.testing_vec_follower.veh[vehicle].acceleration, marker, markersize=markersize)\n",
    "            return(self.sim_traj[particle, vehicle].acceleration - self.testing_vec_follower.veh[vehicle].acceleration)\n",
    "        \n",
    "        \n",
    "    def show_all_particles(self, veh_id, d_type, marker, markersize):\n",
    "        for i in range(self.n_particles):\n",
    "            self.error_plot(i, veh_id, d_type, marker, markersize)\n",
    "    \n",
    "    def show_all_vehs_of_particle(self, particle_select, d_type, marker, markersize):\n",
    "        for i in range(self.testing_samples):\n",
    "            self.error_plot(particle_select, i, d_type, marker, markersize)\n",
    "    \n",
    "    def show_nth_best_veh_of_particle(self, particle_id, n, d_type, marker, markersize):\n",
    "        veh = self.get_veh_quantile(particle_id, n)\n",
    "        self.error_plot(particle_id, veh, d_type, marker, markersize)\n",
    "        return veh\n",
    "                   \n",
    "    def show_nth_best_particle_of_veh(self, veh_id, n, d_type, marker):\n",
    "        particle_select = self.get_particle_quantile(veh_id, n)\n",
    "        self.error_plot(particle_select, veh_id, d_type, marker, 1)\n",
    "        return particle_select\n",
    "    \n",
    "    def show_best_n_particle_of_veh(self, veh_id, n, d_type, marker):\n",
    "        plot_result = {}\n",
    "        for i in range(n):\n",
    "            particle_select = self.get_particle_quantile(veh_id, i)\n",
    "            plot_result[i] = self.error_plot(particle_select, veh_id, d_type, marker, 1)\n",
    "        return plot_result\n",
    "        \n",
    "    def show_matched_particles_of_veh(self, veh_id, d_type, marker):\n",
    "        n_list = np.where(self.X[:,veh_id]>1e-5)[0]\n",
    "        for i in n_list:\n",
    "            particle_select = i\n",
    "            self.error_plot(particle_select, veh_id, d_type, marker, 1)\n",
    "    def show_matched_particles_weight_of_veh(self, veh_id, d_type, marker):\n",
    "        for i in range(self.n_particles):\n",
    "            particle_select = i\n",
    "            self.error_plot(particle_select, veh_id, d_type, marker, markersize = self.X[i,veh_id]/100)\n",
    "       \n",
    "    def show_mean_particle_of_veh(self, veh_id, d_type, marker):\n",
    "        self.error_plot(self.mean_particle, veh_id, d_type, marker, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed3ebf9f-0d9c-44ff-acb2-c4289046f8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = [0.5, 0.3, 0.2]\n",
    "#weight = [1, 0, 0]\n",
    "#my_eval = particle_evaluation('merged_result2_%s.csv' % Data_set,\n",
    "my_eval = particle_evaluation('Single_result_%s_%s.csv' % (Data_set,  model_name),\n",
    "                              '../data/%s/Trajectory/' % Data_set,\n",
    "                              '../data/%s/Testing/' % Data_set)\n",
    "                              #'../data/%s/Trajectory/' % Data_set)\n",
    "my_eval.processing_particles_result(weight)\n",
    "my_eval.cutting_particles(3) ##default as 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee68b985-f360-45db-a59c-3c6a04b89abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96907d0-a856-45d3-9ef6-8ad81a8a224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_eval.load_testing_data()\n",
    "tic = timeit.default_timer()\n",
    "my_eval.particle_testing()\n",
    "toc = timeit.default_timer()\n",
    "print('Time consumption (Particle Testing) %d sec ' % (toc - tic))\n",
    "tic = timeit.default_timer()\n",
    "my_eval.select_mean_particle()\n",
    "toc = timeit.default_timer()\n",
    "print('Time consumption (Mean Particle Selection) %d sec ' % (toc - tic))\n",
    "my_eval.cal_wasserstein_dist()\n",
    "my_eval.cal_minimum_dist()\n",
    "my_eval.cal_five_percent_dist() \n",
    "#my_eval.std()\n",
    "#print(my_eval.five_percent_dist)\n",
    "print(my_eval.mean_particle_test_score, my_eval.minimum_dist, my_eval.five_percent_dist, my_eval.wasserstein_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf306f4-dc35-4cbc-8e02-25a14f9e252a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24506d5d-f0f6-4d93-af36-4a62b57d6be2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec26d00-c754-42fd-a09e-afea126ff71c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "veh = 2\n",
    "LABEL = {'p': 'Position Error (m)', 's': 'Speed Error (m/s)', 'a':''}\n",
    "def show_fitted_result(my_eval, veh, d_type, yr):\n",
    "    \n",
    "    #my_eval.show_mean_particle_of_veh(veh,d_type,'r')\n",
    "    # my_eval.show_matched_particles_of_veh(veh,d_type,'b.')\n",
    "    #my_eval.show_nth_best_particle_of_veh(veh,0,d_type,'b')\n",
    "    #my_eval.show_nth_best_particle_of_veh(veh,5,d_type,'g')\n",
    "    my_eval.show_best_n_particle_of_veh(veh,5,d_type,'r')\n",
    "    \n",
    "    # my_eval.show_all_particles_testing_veh(veh)\n",
    "    # plt.ylim(yr)\n",
    "    \n",
    "    # plt.show()\n",
    "    plt.plot([0,0],[0,0],'y')\n",
    "    my_eval.show_all_particles(veh,d_type, 'y.', .04)\n",
    "    #plt.legend(['Top 5% Best Fitted Particles of Hybrid model','All Particles'])  #,  \n",
    "    plt.ylim(yr)\n",
    "    plt.xlabel('Time Interval (0.1 sec)')\n",
    "    plt.ylabel(LABEL[d_type])\n",
    "    #plt.savefig(d_type + '_veh_%d_%s.jpg' % (veh, Data_set), dpi = 1200) #model_name\n",
    "    #plt.savefig(d_type + '_veh_%d_hybrid_Model_Data_%s_%s.jpg' % (veh, Data_set, model_name), dpi = 1200) #model_name,\n",
    "    plt.show()\n",
    "\n",
    "show_fitted_result(my_eval, veh, 'p', [-15, 15])\n",
    "#show_fitted_result(my_eval, veh, 's', [-8, 8])\n",
    "show_fitted_result(my_eval, veh, 'p', [-10, 10])\n",
    "#show_fitted_result(my_eval, veh, 's', [-10, 10])\n",
    "#print(my_eval.mean_particle_test_score, my_eval.expectation)\n",
    "print(my_eval.wasserstein_dist, my_eval.mean_particle_test_score)\n",
    "#print(my_eval.wasserstein_dist, my_eval.mean_particle_test_score, my_eval.expectation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a3ac4b-d7bd-44df-a592-76e9b3da4ff8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0035e290-bf4c-4151-8db9-c4d251c0caa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "n_dim = 6\n",
    "particles = np.loadtxt('C:/Users/jjiang284/Box/Hybrid model/hybrid/Filtered/tesla/Tesla_analysis_hl.csv', delimiter=\",\", skiprows=0)\n",
    "for i in range(n_dim):\n",
    "    ax=sns.distplot(particles[:,i], bins=10, color='red',\n",
    "    hist_kws={\"edgecolor\": 'black'})\n",
    "    #xlimit = ([0,2],\n",
    "              #[-0.2,1],\n",
    "              #[-0.2,1],\n",
    "              #[-0.2,0.8],\n",
    "              #[-4,-1],\n",
    "              #[2,9])\n",
    "    xlabel = ['tau: Desired time Gap (sec)', 'TT: Actuation lag (sec)', 'Ks: Spacing deviation feedback gain', 'Kv: Speed difference feedback gain','Ka: Acceleration feedback gain','l: standstill distance (m)']  ##HL\n",
    "    #xlabel = ['a: maximum accleration (m/s2)', 'b: desired deceleration (m/s2)', 'Vmax: desired speed (m/s)','T: desired time gap (sec)', 's0: minimum gap (m)', 'delta: free accleration exponent'] #IDM\n",
    "    plt.title('Calibrated',fontsize=18)\n",
    "    plt.xlabel(xlabel[i],fontsize=18)\n",
    "    plt.ylabel('Density',fontsize=18)\n",
    "    plt.savefig('HL_syn_gen_1m{}.jpg'.format(i), dpi = 1200)\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0326ce6c-b570-4b03-8561-cd426d896c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd339e3-ea16-4ae8-9971-1ef646a64cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install cvxopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a846c1e7-bfe6-4fea-9e95-3baa1945b1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_a = np.ones([3,4])\n",
    "test_a[1][1]=0\n",
    "print(test_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d7b2e7-bdfa-4700-9dec-b019ef86518b",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = test_a.sum(axis=1)\n",
    "print(min(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e77a2a-85f3-4eab-86e5-018971c99828",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.amin(score))\n",
    "print(np.where(score == np.amin(score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b5e49c-7d2f-4d6e-a9f6-480c8ac450ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 2, 100)  # Sample data.\n",
    "\n",
    "plt.figure(figsize=(5, 2.7))\n",
    "plt.plot(x, x, 'b',label='Top 5% Best Fitted Particles of GFM')  # Plot some data on the (implicit) axes.\n",
    "plt.plot(x, x**2, 'g', label='Top 5% Best Fitted Particles of HL')  # etc.\n",
    "plt.plot(x, x**3, 'm', label='Top 5% Best Fitted Particles of IDM')\n",
    "plt.plot(x, x**4, 'y', label='All Particles')\n",
    "\n",
    "plt.xlabel('x label')\n",
    "plt.ylabel('y label')\n",
    "plt.title(\"Simple Plot\")\n",
    "plt.legend();\n",
    "plt.savefig('legends.jpg', dpi = 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc801247-48a6-4fbe-8326-001b18526be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 2, 100)  # Sample data.\n",
    "\n",
    "plt.figure(figsize=(10, 2.7))\n",
    "plt.plot(x, x, 'r',label='Top 5% Best Fitted Particles of Hybrid model')  # Plot some data on the (implicit) axes.\n",
    "plt.plot(x, x**4, 'y', label='All Particles')\n",
    "\n",
    "plt.xlabel('x label')\n",
    "plt.ylabel('y label')\n",
    "plt.title(\"Simple Plot\")\n",
    "plt.legend();\n",
    "plt.savefig('legends2.jpg', dpi = 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a39ea8a-d578-4734-ba64-54f85af89e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
