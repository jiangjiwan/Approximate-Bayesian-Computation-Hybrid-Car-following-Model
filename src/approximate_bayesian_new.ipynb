{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7751ea51-8566-4afc-9496-f6dc4444f111",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Samples are truncated.\n",
      "8 batches with 125000 Samples for each batch.\n",
      "Time consumption under parallel computing 1038 sec \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import timeit\n",
    "\n",
    "from traj_data import *\n",
    "from models import *\n",
    "\n",
    "\n",
    "class ABC:\n",
    "    def __init__(self, input_real_data, output_real_data, model, use_parallel):\n",
    "        np.random.seed(0)\n",
    "        self.max_para_dim = 10\n",
    "        self.use_parallel = use_parallel\n",
    "        self.real_input = input_real_data\n",
    "        self.real_output = output_real_data\n",
    "        self.model = model\n",
    "        self.parameter_dim = model.nPara\n",
    "        self.parameter_name = model.strPara\n",
    "        \n",
    "    def load_priori(self,parameter_samples):\n",
    "        if parameter_samples.shape[1] != self.parameter_dim:\n",
    "            raise ValueError('Trajectory Dimension Not Matching')\n",
    "        \n",
    "        self.parameter_samples = np.concatenate((parameter_samples, np.zeros([parameter_samples.shape[0],self.max_para_dim - parameter_samples.shape[1]])), axis = 1)\n",
    "        self.num_samples = self.parameter_samples.shape[0]\n",
    "        \n",
    "    \n",
    "    def run(self):\n",
    "        using_parallel = self.use_parallel\n",
    "        num_cores = multiprocessing.cpu_count()\n",
    "        self.error = {}     \n",
    "        \n",
    "        if using_parallel:\n",
    "            def process(n):\n",
    "                sim_output = self.model.batch_simulation(self.real_input, self.real_output, self.parameter_samples[n,:])          \n",
    "                return sim_output.distance(self.real_output)\n",
    "\n",
    "            self.error = Parallel(n_jobs=num_cores)(delayed(process)(n) for n in range(self.num_samples))\n",
    "        else:                                     \n",
    "            for n in range(self.num_samples):\n",
    "                sim_output = self.model.batch_simulation(self.real_input, self.real_output, self.parameter_samples[n,:])\n",
    "                self.error[n] = sim_output.distance(self.real_output)\n",
    "        \n",
    "        self.parameter_samples = np.concatenate((np.zeros([self.num_samples, 5])+np.Inf, self.parameter_samples), axis = 1)\n",
    "        for n in range(self.num_samples):\n",
    "            self.parameter_samples[n,0] = self.error[n][0]\n",
    "            self.parameter_samples[n,1] = self.error[n][1]\n",
    "            self.parameter_samples[n,2] = self.error[n][2]\n",
    " \n",
    "            self.parameter_samples[loc,3] = sample_id_list[loc]\n",
    "            self.parameter_samples[loc,4] = self.model.model_id\n",
    "    \n",
    "        return self.parameter_samples\n",
    "    \n",
    "    def run_downsample(self, sample_id_list, batchsize):\n",
    "        using_parallel = self.use_parallel\n",
    "        n_batch = int(np.floor(self.num_samples/batchsize))\n",
    "        if n_batch * batchsize > self.num_samples:\n",
    "            raise ValueError('Sample Dimension Not Matching')\n",
    "        if n_batch * batchsize < self.num_samples:\n",
    "            print('%d Samples are truncated.' % (self.num_samples - n_batch * batchsize))\n",
    "        \n",
    "        print('%d batches with %d Samples for each batch.' % (n_batch, batchsize))\n",
    "        num_cores = multiprocessing.cpu_count()\n",
    "        self.error = {}     \n",
    "        \n",
    "        if using_parallel:\n",
    "            def process(n):\n",
    "                list_error = []\n",
    "                for bn in range(batchsize):\n",
    "                    loc = n*batchsize + bn\n",
    "                    sim_output = self.model.downsample_simulation(self.real_input, self.real_output, self.parameter_samples[loc,:], sample_id_list[loc])          \n",
    "                    list_error.append(sim_output.distance(self.real_output))  \n",
    "                return list_error\n",
    "            self.error = Parallel(n_jobs=num_cores)(delayed(process)(n) for n in range(n_batch))\n",
    "        else: \n",
    "            for n in range(n_batch):\n",
    "                list_error = []\n",
    "                for bn in range(batchsize):\n",
    "                    loc = n*batchsize + bn\n",
    "                    sim_output = self.model.downsample_simulation(self.real_input, self.real_output, self.parameter_samples[loc,:], sample_id_list[loc])\n",
    "                    list_error.append(sim_output.distance(self.real_output))\n",
    "                self.error[n] = list_error\n",
    "\n",
    "        \n",
    "        self.parameter_samples = np.concatenate((np.zeros([self.num_samples, 5])+np.Inf, self.parameter_samples), axis = 1)\n",
    "        for n in range(n_batch):\n",
    "            for bn in range(batchsize):\n",
    "                loc = n*batchsize + bn\n",
    "                \n",
    "                self.parameter_samples[loc,0] = self.error[n][bn][0]\n",
    "                self.parameter_samples[loc,1] = self.error[n][bn][1]\n",
    "                self.parameter_samples[loc,2] = self.error[n][bn][2]\n",
    "                \n",
    "                self.parameter_samples[loc,3] = sample_id_list[loc]\n",
    "                self.parameter_samples[loc,4] = self.model.model_id\n",
    "            \n",
    "        return self.parameter_samples\n",
    "    \n",
    "    def save_result(self, file):\n",
    "        header = ''      \n",
    "        header += 'Error_p, Error_s, Error_a,'\n",
    "        header += 'Traj_id, Model_id'\n",
    "        for i in range(self.parameter_dim):\n",
    "            header += ',' + self.parameter_name[i] \n",
    "        for i in range(self.parameter_dim, self.max_para_dim):\n",
    "            header += ', _'\n",
    "        \n",
    "        \n",
    "        self.parameter_samples = self.parameter_samples[self.parameter_samples[:,0].argsort()]\n",
    "        \n",
    "        np.savetxt(file, \n",
    "                   self.parameter_samples, \n",
    "                   delimiter = \",\", fmt='%.4f', \n",
    "                   header = header)\n",
    "\n",
    "def uniform_priori_gen(N, bd_file):\n",
    "    bd = np.loadtxt(open(bd_file, \"rb\"), delimiter=\",\", skiprows=0)\n",
    "    model_dimension = bd.shape[0]\n",
    "    parameter_samples = np.zeros((N, model_dimension))\n",
    "    for n in range(N):\n",
    "        for m in range(model_dimension):\n",
    "            parameter_samples[n,m] = np.random.uniform(bd[m,0], bd[m,1])  \n",
    "    \n",
    "    return parameter_samples\n",
    "\n",
    "def shell(model_name, traj_data_path, para_priori_file, result_path, ts, num_samples):\n",
    "    \n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    batchsize = int(np.floor(num_samples / num_cores))\n",
    "    use_downsample = True\n",
    "    use_parallel = True\n",
    "    \n",
    "    leader_vec, follower_vec = read_traj(traj_data_path, ts)\n",
    "    parameter_samples = uniform_priori_gen(num_samples, para_priori_file) \n",
    "    my_model = MODEL[model_name]()\n",
    "    my_ABC = ABC(leader_vec, follower_vec, my_model, use_parallel)\n",
    "    my_ABC.load_priori(parameter_samples)\n",
    "    \n",
    "    tic = timeit.default_timer()\n",
    "    if use_downsample:\n",
    "        sample_id_list = np.random.randint(0, leader_vec.num_veh, num_samples)\n",
    "        my_ABC.run_downsample(sample_id_list, batchsize)\n",
    "        my_ABC.save_result(result_path)\n",
    "  \n",
    "    else:   \n",
    "        my_ABC.run()\n",
    "        my_ABC.save_result(result_path)\n",
    "        \n",
    "    toc = timeit.default_timer()\n",
    "    if use_parallel:\n",
    "        print('Time consumption under parallel computing %d sec ' % (toc - tic))\n",
    "    else:\n",
    "        print('Time consumption under serial computing %d sec' % (toc - tic))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    num_samples = 1000001\n",
    "    ts = 0.1\n",
    "    Data_set = 'I_80_2'\n",
    "    traj_data_path = '../data/%s/Trajectory/' % Data_set\n",
    "    \n",
    "    for model_name in ['GFM']: ## ', ,     'IDM', 'FVM', 'GFM', 'OVM' ,'LL','HL', 'LLCS'  FVM_CS' ,'Newell','IDM_CS','IDM_CTG',FVM_SIGMOID\n",
    "        para_priori_file = '../priori/%s_15_uniform.csv' % model_name\n",
    "        #result_path = '../Cross_result_%d_%s_%s_new.csv' % (num_samples, model_name, Data_set)\n",
    "        result_path = '../New_15_result_%d_%s_%s_new3.csv' % (num_samples, model_name, Data_set)\n",
    "\n",
    "        shell(model_name, traj_data_path, para_priori_file, result_path, ts, num_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afd80f0-971c-4266-a7e5-180b91e5164e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7387bc02-2a12-4029-b91d-6a78add052a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aeaf7a-bc2c-4917-9def-ea377a6f092c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
